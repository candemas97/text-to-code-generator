{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiomora03/AdvancedTopicsAnalytics/blob/main/exercises/E7-TextSummary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Traducción de Texto**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install transformers\n",
        "# pip install sentencepiece\n",
        "# pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import transformers\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "pregunta = \"Como hago una funcion lineal?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "StIrHeBAB07H"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
            "\n",
            "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-es-en.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# Se entrena modelo con traductor español - inglés\n",
        "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-es-en\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How do I make a linear function?\n"
          ]
        }
      ],
      "source": [
        "# Se traduce la oración\n",
        "english_quesion = translator(\n",
        "    pregunta, clean_up_tokenization_spaces=True, truncation=True\n",
        ")\n",
        "print(english_quesion[0][\"translation_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Función que traduce cualquier texto del español al inglés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
            "\n",
            "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-es-en.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# Se entrena modelo con traductor español - inglés\n",
        "translator = pipeline(\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "def traductor(pregunta: str) -> str:\n",
        "    \"\"\"Tranducir una frase que se ingrese del español al inglés\n",
        "\n",
        "    Args:\n",
        "        pregunta (str): Pregunta en español a ser traducida\n",
        "\n",
        "    Returns:\n",
        "        str: Pregunta en inglés\n",
        "    \"\"\"\n",
        "\n",
        "    english_quesion = translator(\n",
        "        pregunta, clean_up_tokenization_spaces=True, truncation=True\n",
        "    )\n",
        "    return english_quesion[0][\"translation_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Using Pretrained model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "class Args:\n",
        "    # define training arguments\n",
        "\n",
        "    # MODEL\n",
        "    model_type = 't5'\n",
        "    tokenizer_name = 'Salesforce/codet5-base'\n",
        "    model_name_or_path = 'Salesforce/codet5-base'\n",
        "\n",
        "    # DATA\n",
        "    train_batch_size = 8\n",
        "    validation_batch_size = 8\n",
        "    max_input_length = 48\n",
        "    max_target_length = 128\n",
        "    prefix = \"Generate Python: \"\n",
        "\n",
        "    # OPTIMIZER\n",
        "    learning_rate = 3e-4\n",
        "    weight_decay = 1e-4\n",
        "    warmup_ratio = 0.2\n",
        "    adam_epsilon = 1e-8\n",
        "\n",
        "    # TRAINING\n",
        "    seed = 2022\n",
        "    epochs = 20\n",
        "\n",
        "    # DIRECTORIES\n",
        "    output_dir = \"runs/\"\n",
        "    logging_dir = f\"{output_dir}/logs/\"\n",
        "    checkpoint_dir = f\"checkpoint\"\n",
        "    save_dir = f\"{output_dir}/saved_model/\" #HERE YOU MUST ADD THE COMPLETE PATH\n",
        "    cache_dir = '../working/'\n",
        "    #Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    #Path(logging_dir).mkdir(parents=True, exist_ok=True)\n",
        "    #Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# initialize training arguments\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_predict(args, text):\n",
        "    # load saved finetuned model\n",
        "    model = transformers.TFT5ForConditionalGeneration.from_pretrained(args.save_dir)\n",
        "    # load saved tokenizer\n",
        "    tokenizer = transformers.RobertaTokenizer.from_pretrained(args.save_dir)\n",
        "\n",
        "     # encode texts by prepending the task for input sequence and appending the test sequence\n",
        "    query = args.prefix + text\n",
        "    encoded_text = tokenizer(query, return_tensors='tf', padding='max_length', truncation=True, max_length=args.max_input_length)\n",
        "\n",
        "    # inference\n",
        "    generated_code = model.generate(\n",
        "        encoded_text[\"input_ids\"], attention_mask=encoded_text[\"attention_mask\"],\n",
        "        max_length=args.max_target_length, top_p=0.95, top_k=50, repetition_penalty=2.0, num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    # decode generated tokens\n",
        "    decoded_code = tokenizer.decode(generated_code.numpy()[0], skip_special_tokens=True)\n",
        "    return decoded_code\n",
        "\n",
        "def predict_from_dataset(args):\n",
        "    # load using hf datasets\n",
        "    dataset = load_dataset('json', data_files='../working/mbpp.jsonl')\n",
        "    # train test split\n",
        "    dataset = dataset['train'].train_test_split(0.1, shuffle=False)\n",
        "    test_dataset = dataset['test']\n",
        "\n",
        "    # randomly select an index from the validation dataset\n",
        "    index = random.randint(0, len(test_dataset))\n",
        "    text = test_dataset[index]['text']\n",
        "    code = test_dataset[index]['code']\n",
        "\n",
        "    # run-predict on text\n",
        "    decoded_code = run_predict(args, text)\n",
        "\n",
        "    print(\"#\" * 25); print(\"QUERY: \", text);\n",
        "    print()\n",
        "    print('#' * 25); print(\"ORIGINAL: \"); print(\"\\n\", code);\n",
        "    print()\n",
        "    print('#' * 25); print(\"GENERATED: \"); print(\"\\n\", decoded_code);\n",
        "\n",
        "def predict_from_text(args, text):\n",
        "    # run-predict on text\n",
        "    decoded_code = run_predict(args, text)\n",
        "    print(\"#\" * 25); print(\"QUERY: \", text);\n",
        "    print()\n",
        "    print('#' * 25); print(\"GENERATED: \"); print(\"\\n\", decoded_code);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Paso a Paso Backend**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at C:/Users/cande/Downloads/runs/saved_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
            "c:\\Users\\cande\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#########################\n",
            "QUERY:  Write a function to add two random numbers\n",
            "\n",
            "#########################\n",
            "GENERATED: \n",
            "\n",
            " def add_random(num1, num2):\n",
            "    random = [0 for i in range (min(_int(),max((x) + 1)/3)] \n",
            "        if x % 2 == 0:  \n",
            "            return y    \n",
            "    else :\n",
            "          None;\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# example 1\n",
        "predict_from_text(args, \"Write a function to add two random numbers\"); print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main_IA_backend():\n",
        "    \"\"\"\n",
        "    Función que realiza todo el cómputo para generar la generación de código.\n",
        "\n",
        "    Está disponible para inglés y español\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        idioma = input(\"Select your language (English or Spanish)\")\n",
        "        if str(idioma.lower()) == \"english\":\n",
        "            question = str(input(\"Write down the function you want to create\"))\n",
        "            predict_from_text(args, question); print()\n",
        "            break\n",
        "        elif str(idioma.lower()) == \"spanish\":\n",
        "            pregunta = str(input(\"Escribe la función que deseas crear\"))\n",
        "            question = traductor(pregunta)\n",
        "            print(question)\n",
        "            predict_from_text(args, question); print()\n",
        "            break\n",
        "        else:\n",
        "            print(\"Just two possible languages (English or Spanish). Try Again.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Just two possible languages (English or Spanish). Try Again.\n",
            "Just two possible languages (English or Spanish). Try Again.\n",
            "Just two possible languages (English or Spanish). Try Again.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at C:/Users/cande/Downloads/runs/saved_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
            "c:\\Users\\cande\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#########################\n",
            "QUERY:  Create a function that substract two values\n",
            "\n",
            "#########################\n",
            "GENERATED: \n",
            "\n",
            " def substract_single(a,b):\n",
            "    if a > b: \n",
            "        return (A-B)  \n",
            "    res = A + B - 1\n",
            "    for i in range(_mini(), _maxint), 0, 2 :    \n",
            "         result += ((c * d)+((e % p ==0 or e%p!= n))          ]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "main_IA_backend()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMk/du9ggsuzmUoZg99Ampu",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
