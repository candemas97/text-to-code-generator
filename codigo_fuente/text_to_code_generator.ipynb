{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"title_ID\"></a>\n",
        "<font color='#425066'><center><h1>Text-to-code Generation with <span style=\"color:#FF6F00\">TensorFlow</span>, ü§ó & MBPP</h1></center></font>\n",
        "\n",
        "This notebook demonstrates how to generate python code based on the natural language description using [TensorFlow](https://www.tensorflow.org/), [HuggingFace](https://huggingface.co/) and [Mostly Basic Python Programming Benchmark](https://research.google/tools/datasets/mostly-basic-python-problems/) from [Google Research](https://research.google/).\n",
        "\n",
        "<a id=\"section1\"><font color='#FF6F00'><h2>Introduction</h2></font></a>\n",
        "\n",
        "Given the goal of improving software development productivity with machine learning methods, software intelligence research has attracted increasing attention in both academia and industries over the last decade. Software code intelligence techniques can help developers reduce tedious repetitive workloads, enhance the programming quality, and improve the overall software development productivity. This would reduce time spent writing software as well as reduce computational and operational costs.\n",
        "\n",
        "[Text-to-code generation](https://paperswithcode.com/task/text-to-code-generation) is a task where we can generate code based on the natural language description. It can further be used to build an AI-powered coding assistant. Developers simply type the natural language description or the function signature to specify their intents, and the AI coding assistant can generate or complete the target function for them. This helps to accelerate implementation and also reduce their reliance on external resources.\n",
        "\n",
        "<br>\n",
        "<center><img src=\"https://production-media.paperswithcode.com/tasks/code_generation_0P9CcWa.png\" alt=\"code_generation\" width=\"600\"/></center>\n",
        "<br>\n",
        "\n",
        "[CodeT5 by Salesforce](https://arxiv.org/pdf/2109.00859.pdf) is the first code-aware, encoder-decoder-based pre-trained programming language model, which enables a wide range of code intelligence applications including code understanding and generation tasks. CodeT5 achieves state-of-the-art performance on 14 sub-tasks in the CodeXGLUE code intelligence benchmark.\n",
        "\n",
        "CodeT5 builds on an encoder-decoder framework with the same architecture as [T5 by Google](https://arxiv.org/pdf/1910.10683.pdf). The T5 model, pre-trained on [C4](https://www.tensorflow.org/datasets/catalog/c4), achieves state-of-the-art results on many NLP benchmarks while being flexible enough to be fine-tuned to a variety of important downstream tasks. T5 architecture employs denoising sequence-to-sequence (Seq2Seq) pre-training and has been shown to benefit out-of-the-box for both understanding and generation tasks in natural language.\n",
        "\n",
        "In this notebook we will finetune CodeT5 on [MBPP - Mostly Basic Python Problems by Google Research](https://research.google/tools/datasets/mostly-basic-python-problems/) to generate code based on problem description. MBPP is a benchmark that consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases.\n",
        "\n",
        "<font color=#425066><b>The notebook demonstrates how to finetune CodeT5 on MBPP Dataset w/ TensorFlow.</b></font>\n",
        "\n",
        "<style>\n",
        "a:link {\n",
        "  color: green;\n",
        "  background-color: transparent;\n",
        "  text-decoration: none;\n",
        "}\n",
        "</style>\n",
        "\n",
        "<font color='#FF6F00'><h2>Table of Contents</h2></font>\n",
        "\n",
        "- [<font color=#FF6F00><b>Introduction</b></font>](#section1)\n",
        "\n",
        "- [<font color=#FF6F00><b>T5</b></font>](#section2)\n",
        "  - [<font color=#425066>Explained</font>](#section2a)  \n",
        "  - [<font color=#425066>Training</font>](#section2b)  \n",
        "  - [<font color=#425066>Inference</font>](#section2c)  \n",
        "  - [<font color=#425066>References</font>](#section2d)  \n",
        "  \n",
        "- [<font color=#FF6F00><b>CodeT5</b></font>](#section3)\n",
        "  - [<font color=#425066>Explained</font>](#section3a)  \n",
        "  - [<font color=#425066>Pretraining</font>](#section3b)  \n",
        "  - [<font color=#425066>Tokenization</font>](#section3c)  \n",
        "  - [<font color=#425066>References</font>](#section3d)  \n",
        "  \n",
        "- [<font color=#FF6F00><b>Imports</b></font>](#section4)\n",
        "\n",
        "- [<font color=#FF6F00><b>Setup Strategy</b></font>](#section5)  \n",
        "  - [<font color=#425066>Mixed Precision Training</font>](#section5a)\n",
        "  - [<font color=#425066>XLA: Optimizing Compiler for Machine Learning</font>](#section5b)\n",
        "  - [<font color=#425066>Distributed Training</font>](#section5c)\n",
        "\n",
        "- [<font color=#FF6F00><b>MBPP Benchmark</b></font>](#section6)\n",
        "- [<font color=#FF6F00><b>Data Processing</b></font>](#section7)\n",
        "- [<font color=#FF6F00><b>Utility Functions / Class</b></font>](#section8)\n",
        "- [<font color=#FF6F00><b>Custom Training Loop</b></font>](#section9)\n",
        "- [<font color=#FF6F00><b>Run</b></font>](#section10)\n",
        "- [<font color=#FF6F00><b>Execute</b></font>](#section11)\n",
        "- [<font color=#FF6F00><b>Predict</b></font>](#section12)\n",
        "  - [<font color=#425066>Predict from Dataset</font>](#section12a)  \n",
        "  - [<font color=#425066>Predict from Text</font>](#section12b)\n",
        "- [<font color=#FF6F00><b>Ending Notes</b></font>](#section13)\n",
        "- [<font color=#FF6F00><b>References</b></font>](#section14)\n",
        "\n",
        "\n",
        "<a id=\"section2\"><font color='#FF6F00'><h2>T5</h2></font></a>\n",
        "\n",
        "<a id=\"section2a\"><font color=#425066><h3>Explained</h3></font></a>\n",
        "\n",
        "- *T5 ‚Äî Text-to-Text Transfer Transformer Model* was proposed in the paper, [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683). This paper is essentially a survey of modern transfer learning techniques used in language understanding and hence proposes a unified framework that attempts to combine all language problems into a text-to-text format.\n",
        "\n",
        "- The text-to-text framework suggests using the same model, same loss function, and the same hyperparameters on all the NLP tasks. In this approach, the inputs are modeled in such a way that the model shall recognize a task, and the output is simply the ‚Äútext‚Äù version of the expected outcome.\n",
        "\n",
        "<br>\n",
        "<center><img src=\"https://miro.medium.com/max/1280/0*xfXDPjASztwmJlOa.gif\" alt=\"t5\" width=\"600\"/></center>\n",
        "<br>\n",
        "\n",
        "- To avail the same model for all the downstream tasks, a task-specific text prefix is added to the original input that is fed to the model. This text prefix is also considered as a hyperparameter.\n",
        "> As an example,to ask the model to translate the sentence ‚ÄúThat is good.‚Äù from English to German, the model would be fed the sequence ‚Äútranslate English to German: That is good.‚Äù and would be trained to output ‚ÄúDas ist gut.‚Äù\n",
        "‚Äî T5 Paper\n",
        "\n",
        " Similarly, for classification tasks, the model predicts a single word corresponding to the target label.\n",
        "> For example, on the MNLI benchmark the goal is to predict whether a premise implies (‚Äúentailment‚Äù), contradicts (‚Äúcontradiction‚Äù), or neither (‚Äúneutral‚Äù) a hypothesis. With our preprocessing, the input sequence becomes ‚Äúmnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.‚Äù with the corresponding target word ‚Äúentailment‚Äù.\n",
        "‚Äî T5 Paper\n",
        "\n",
        "- T5 is pretrained using the denoising objective on C4‚Äî Colossal Clean Crawled Corpus - a 750GB dataset which is not just reasonably larger than the most pre-training datasets but also contains a relatively very clean text.\n",
        "\n",
        "<br>\n",
        "<center><img src=\"https://miro.medium.com/max/1280/0*8WIzJDlbitZVDGeA.png\" alt=\"t5_pretrain_finetune\" width=\"600\"/></center>\n",
        "<br>\n",
        "\n",
        "- The proposed model is essentially a Encoder-Decoder [Transformer](https://arxiv.org/abs/1706.03762) with some architectural changes (like applying Layer Normalization before a sub block and then adding the initial input to the sub-block output; also known as pre-norm). Moreover, the model configuration is similar to [BERT base](https://arxiv.org/abs/1810.04805). T5 uses relative scalar embeddings. Encoder input padding can be done on the left and on the right.\n",
        "\n",
        "- T5 comes in different sizes: t5-small, t5-base, t5-large, t5-3b, t5-11b.\n",
        "\n",
        "<a id=\"section2b\"><font color=#425066><h3>Training</h3></font></a>\n",
        "\n",
        "- T5 is an encoder-decoder model and converts all NLP problems into a text-to-text format. It is trained using teacher forcing. This means that for training, we always need an input sequence and a corresponding target sequence. The input sequence is fed to the model using `input_ids`. The target sequence is shifted to the right, i.e., prepended by a start-sequence token and fed to the decoder using the `decoder_input_ids`. In teacher-forcing style, the target sequence is then appended by the EOS token and corresponds to the `labels`. The PAD token is hereby used as the start-sequence token. T5 can be trained / fine-tuned both in a supervised and unsupervised fashion.\n",
        "\n",
        "- One can use [T5ForConditionalGeneration](https://huggingface.co/docs/transformers/v4.18.0/en/model_doc/t5#transformers.T5ForConditionalGeneration) (or the Tensorflow/Flax variant) from [HuggingFace transformers library](https://huggingface.co/docs/transformers/), which includes the language modeling head on top of the decoder.\n",
        "\n",
        "- T5 models need a slightly higher learning rate than the default one set in the Trainer when using the AdamW optimizer. Typically, 1e-4 and 3e-4 work well for most problems (classification, summarization, translation, question answering, question generation). Note that T5 was pre-trained using the AdaFactor optimizer.\n",
        "\n",
        "- Task prefixes matter when (1) doing multi-task training (2) your task is similar or related to one of the supervised tasks used in T5‚Äôs pre-training mixture (see Appendix D of the [paper](https://arxiv.org/pdf/1910.10683.pdf) for the task prefixes used).\n",
        "\n",
        "-  We must make sure that padding token id‚Äôs of the labels are not taken into account by the loss function. In PyTorch and Tensorflow, this can be done by replacing them with -100, which is the `ignore_index` of the `CrossEntropyLoss`. We also pass attention_mask as additional input to the model, which makes sure that padding tokens of the inputs are ignored.\n",
        "\n",
        "<a id=\"section2c\"><font color=#425066><h3>Inference</h3></font></a>\n",
        "\n",
        "- At inference time, it is recommended to use [generate()](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate). This method takes care of encoding the input and feeding the encoded hidden states via cross-attention layers to the decoder and auto-regressively generates the decoder output. Check out this [blog post](https://huggingface.co/blog/how-to-generate) to know all the details about generating text with Transformers. There‚Äôs also this [blog post](https://huggingface.co/blog/encoder-decoder#encoder-decoder) which explains how generation works in general in encoder-decoder models.\n",
        "\n",
        "<a id=\"section2d\"><font color=#425066><h3>References</h3></font></a>\n",
        "- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)\n",
        "- [Google AI Blog - Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)\n",
        "- [HuggingFace - T5](https://huggingface.co/docs/transformers/model_doc/t5#training)\n",
        "- [Understanding Transformer-Based Self-Supervised Architectures](https://towardsdatascience.com/t5-text-to-text-transfer-transformer-643f89e8905e)\n",
        "- [T5 - A Detailed Explanation](https://medium.com/analytics-vidhya/t5-a-detailed-explanation-a0ac9bc53e51)\n",
        "\n",
        "\n",
        "<a id=\"section3\"><font color='#FF6F00'><h2>CodeT5</h2></font></a>\n",
        "\n",
        "<a id=\"section3a\"><font color=#425066><h3>Explained</h3></font></a>\n",
        "\n",
        "-  CodeT5 by Salesforce was proposed in [CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models\n",
        "for Code Understanding and Generation](https://arxiv.org/pdf/2109.00859.pdf) and is an open-source model that can understand and readily generate code. It is an identifier-aware unified pre-trained coder-encoder tool that enables a wide range of code intelligence applications. CodeT5 possesses an uninformed model for natural language processing tasks, which reframes text-to-text with input, and output data always being strings of texts.\n",
        "\n",
        "<br>\n",
        "<center><img src=\"https://raw.githubusercontent.com/salesforce/codet5/master/CodeT5.png\" alt=\"codet5\" width=\"550\"/></center>\n",
        "<br>\n",
        "\n",
        "- CodeT5 builds on the similar architecture of T5 but incorporates code-specific knowledge to endow the model with better code understanding. It takes code and its accompanying comments as a sequence to build and generate upon. It aims to derive generic representations for programming language (PL) and natural language (NL) via pre-training on unlabeled source code.\n",
        "\n",
        "- CodeT5 achieves state-of-the-art performance on multiple code-related downstream tasks including understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL.\n",
        "\n",
        "\n",
        "<a id=\"section3b\"><font color=#425066><h3>Pretraining</h3></font></a>\n",
        "\n",
        "- CodeT5 was pretrained on the [CodeSearchNet](https://github.com/github/CodeSearchNet) data that consists of both unimodal (PL-only) and bimodal (PL-NL) data on six PLs - `Ruby, JavaScript, Go, Python, PHP, C, and C#`. In addition to that, they further collect extra data of C/C# from open-source Github repositories. They further finetune CodeT5 on most tasks in the [CodeXGLUE benchmark](https://arxiv.org/pdf/2102.04664.pdf), including two understanding tasks: code defect detection and clone detection, and generation tasks such as code summarization, generation, translation, and refinement.\n",
        "\n",
        "<br>\n",
        "<center><img src=\"https://149695847.v2.pressablecdn.com/wp-content/uploads/2021/09/image-179-1024x406.png\" alt=\"codet5_pretraining\" width=\"700\"/></center>\n",
        "<br>\n",
        "\n",
        "- Some of the pre-training tasks of CodeT5 include:\n",
        "\n",
        "    - `Masked Span Prediction (MSP)` randomly masks spans with arbitrary lengths and requires the decoder to recover the original input. It captures the syntactic information of the NL-PL input and learns robust cross-lingual representations as we pre-train on multiple PLs with a shared model.\n",
        "    \n",
        "    - `Identifier Tagging (IT)` applied only to the encoder which distinguishes whether each code token is an identifier (e.g., variables or function names) or not. It works like the syntax highlighting feature in some developer-aided tools.\n",
        "    \n",
        "    - `Masked Identifier Prediction (MIP)`, in contrast to MSP, only masks identifiers and employs the same mask placeholder for all occurrences of one unique identifier. It works like deobfuscation in software engineering and is a more challenging task that requires the model to comprehend the code semantics based on the obfuscated code.\n",
        "    \n",
        "    - `Bimodal Dual Generation (dual-gen)` jointly optimizes the conversion from code to its comments and vice versa. It encourages a better alignment between the NL and PL counterparts.\n",
        "    \n",
        "<a id=\"section3c\"><font color=#425066><h3>Code-specific Tokenizer</h3></font></a>\n",
        "\n",
        "- In CodeT5 they train a Byte-level BPE tokenizer and set the vocabulary size to 32,000 as T5. They add additional special tokens ([PAD], [CLS], [SEP], [MASK0], ..., [MASK99]). This tokenzier is trained on all pre-training data with non-printable characters and low-frequent tokens (occurring <3 times) filtered.\n",
        "\n",
        "- When compared to T5‚Äôs default tokenizer they find that the trained tokenizer largely reduces the length of tokenized code sequence by 30% - 45% on downstream tasks. This accelerates the training and especially benefits generation tasks due to the shorter sequence to predict.\n",
        "\n",
        "- They spot a severe problem for applying the T5‚Äôs default tokenizer on source code, where it would encode some common code tokens such as brackets [‚Äò{‚Äô, ‚Äò}‚Äô] into unknown tokens.\n",
        "\n",
        "<a id=\"section3d\"><font color=#425066><h3>References</h3></font></a>\n",
        "- [CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation](https://arxiv.org/pdf/2109.00859.pdf)\n",
        "- [CodeT5 Blog by Salesforce](https://blog.salesforceairesearch.com/codet5/)\n",
        "- [CodeT5 GitHub Repository](https://github.com/salesforce/CodeT5#fine-tuning)\n",
        "- [HuggingFace CodeT5-Base Model](https://huggingface.co/Salesforce/codet5-base)\n",
        "- [Salesforce‚Äôs CodeT5 system can understand and generate code](https://venturebeat.com/2021/09/07/salesforces-codet5-system-can-understand-and-generate-code/)\n",
        "- [Salesforce CodeT5 vs Github Copilot: A Comparative Guide to Auto-code Generators](https://analyticsindiamag.com/salesforce-codet5-vs-github-copilot-a-comparative-guide-to-auto-code-generators/)\n",
        "- [Fine_tune_CodeT5_for_generating_docstrings_from_Ruby_code](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tune_CodeT5_for_generating_docstrings_from_Ruby_code.ipynb)\n",
        "- [CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation](https://arxiv.org/pdf/2102.04664.pdf)\n",
        "- [CodeSearchNet](https://github.com/github/CodeSearchNet)\n",
        "- [OpenAI Codex](https://openai.com/blog/openai-codex/)\n",
        "\n",
        "<a id=\"section4\"><font color='#FF6F00'><h2>Imports</h2></font></a>\n",
        "\n",
        "Let's start by importing required libraries to the environment:\n",
        "\n",
        "- [*TensorFlow*](https://www.tensorflow.org/) an end-to-end open source platform for machine learning.\n",
        "- [*transformers*](https://huggingface.co/docs/transformers/index) provides APIs to easily download and train state-of-the-art pretrained models\n",
        "- [*datasets*](https://huggingface.co/docs/datasets/index) a library for easily accessing and sharing datasets."
      ],
      "metadata": {
        "id": "vWAWvkZt8dYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiMobmVM8nYn",
        "outputId": "66d4f3e7-2dac-4c68-f603-b6d7bb5f7df2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"../input/tf-custom-training-loop-figure/tf_process.png\" \"../working/\""
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-04-21T12:12:57.975766Z",
          "iopub.status.busy": "2022-04-21T12:12:57.975425Z",
          "iopub.status.idle": "2022-04-21T12:12:58.728784Z",
          "shell.execute_reply": "2022-04-21T12:12:58.727871Z",
          "shell.execute_reply.started": "2022-04-21T12:12:57.975662Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXyazDBP8dYS",
        "outputId": "b5edc43b-ed24-4b6e-934e-6fe9a15627ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '../input/tf-custom-training-loop-figure/tf_process.png': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"  # reduce the amount of console output from TF\n",
        "import tensorflow as tf\n",
        "\n",
        "import transformers\n",
        "!pip install -q datasets # install HF datasets library\n",
        "from datasets import load_dataset\n",
        "\n",
        "transformers.logging.set_verbosity_warning()\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "import logging\n",
        "\n",
        "print('TF version',tf.__version__)\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU'))) # check GPU available"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-21T12:12:58.731092Z",
          "iopub.status.busy": "2022-04-21T12:12:58.73064Z",
          "iopub.status.idle": "2022-04-21T12:13:24.838761Z",
          "shell.execute_reply": "2022-04-21T12:13:24.837852Z",
          "shell.execute_reply.started": "2022-04-21T12:12:58.731052Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-quNU2TU8dYT",
        "outputId": "ccf83932-cbcc-4730-dd6b-fe50597de94c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hTF version 2.14.0\n",
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section5\"><font color='#FF6F00'><h2>Setup Strategy</h2></font></a>\n",
        "\n",
        "<a id=\"section5a\"><font color=#425066><h3>Mixed Precision Training</h3></font></a>\n",
        "\n",
        "- Floating point types store numeric information of three types ‚Äì the sign, exponent, and fraction.  Traditional float32 representations have 8 bits and 23 bits respectively to represent the exponent and fraction.  Traditional float16 representations (the format used for NVIDIA hardware) roughly halve both the exponent and fraction components of the representation. TPUs use a variant called bfloat16.\n",
        "\n",
        "- Most of a transformer network can be naively converted to float16 weights and activations with no accuracy penalty.\n",
        "\n",
        "<br>\n",
        "<center><img src=\"https://www.pragmatic.ml/content/images/2020/04/image-2.png\" alt=\"fp16\" width=\"600\"/></center>\n",
        "<br>\n",
        "\n",
        "- Small portions of the network ‚Äì in particular, portions of the softmax operation ‚Äì must remain in float32.  This is because the sum of a large number of small values (our logits) can be a source of accumulated error. Because both float16 and float32 values are used, this method is often referred to as \"mixed-precision\" training.\n",
        "  \n",
        "- Less precise numeric representations enable speedups from two sources.\n",
        " - Native half-precision instructions\n",
        " - Larger batch sizes thanks to more compact representations\n",
        "\n",
        "- Note:\n",
        "   > While mixed precision will run on most hardware, it will only speed up models on recent NVIDIA GPUs and Cloud TPUs. NVIDIA GPUs support using a mix of float16 and float32, while TPUs support a mix of bfloat16 and float32. Among NVIDIA GPUs, those with compute capability 7.0 or higher will see the greatest performance benefit from mixed precision because they have special hardware units, called Tensor Cores, to accelerate float16 matrix multiplications and convolutions. - TensorFlow\n",
        "   \n",
        "<a id=\"section5b\"><font color=#425066><h3>XLA: Optimizing Compiler for Machine Learning</h3></font></a>\n",
        "\n",
        "- XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that can accelerate TensorFlow models with potentially no source code changes.\n",
        "\n",
        "<br>\n",
        "<center><img src=\"https://www.tensorflow.org/xla/images/tf_xla_performance.png\" alt=\"xla\" width=\"500\" height=\"500\"/></center>\n",
        "<br>\n",
        "\n",
        "- When a TensorFlow program is run, all of the operations are executed individually by the TensorFlow executor. Each TensorFlow operation has a precompiled GPU kernel implementation that the executor dispatches to. XLA provides an alternative mode of running models: it compiles the TensorFlow graph into a sequence of computation kernels generated specifically for the given model. Because these kernels are unique to the model, they can exploit model-specific information for optimization. For example, let's look at an optimization XLA does in the context of a simple TensorFlow computation:\n",
        "\n",
        "    ```\n",
        "    def model_fn(x, y, z):\n",
        "      return tf.reduce_sum(x + y * z)\n",
        "    ```\n",
        "\n",
        "- XLA can optimize the graph so that it computes the result in a single kernel launch. It does this by \"fusing\" the addition, multiplication and reduction into a single GPU kernel. Moreover, this fused operation does not write out the intermediate values produced by y*z and x+y*z to memory; instead it \"streams\" the results of these intermediate computations directly to their users while keeping them entirely in GPU registers. Fusion is XLA's single most important optimization.\n",
        "\n",
        "<a id=\"section5c\"><font color=#425066><h3>Distribution Strategy</h3></font></a>\n",
        "- `tf.distribute.Strategy` is a TensorFlow API to distribute training across multiple GPUs, multiple machines, or TPUs. Using this API, we can distribute our existing models and training code with minimal code changes.\n",
        "\n",
        "- `tf.distribute.Strategy` intends to cover a number of use cases along different axes. Some of these axes are:\n",
        "\n",
        "    - Synchronous vs asynchronous training: These are two common ways of distributing training with data parallelism. In sync training, all workers train over different slices of input data in sync, and aggregating gradients at each step. In async training, all workers are independently training over the input data and updating variables asynchronously. Typically sync training is supported via all-reduce and async through parameter server architecture.\n",
        "    - Hardware platform: You may want to scale your training onto multiple GPUs on one machine, or multiple machines in a network (with 0 or more GPUs each), or on Cloud TPUs.\n",
        "    \n",
        "- We can use `tf.distribute.Strategy` with very few changes to our code, because the underlying components of TensorFlow have been changed to become strategy-aware. This includes variables, layers, models, optimizers, metrics, summaries, and checkpoints.\n",
        "\n",
        "- We can execute our programs eagerly, or in a graph using `tf.function`. It intends to support both these modes of execution, but works best with graph execution.\n",
        "\n",
        "- The `tf.distribute.Strategy` classes provide a core set of methods to support custom training loops. Using these may require minor restructuring of the code initially, but once that is done, we should be able to switch between GPUs, TPUs, and multiple machines simply by changing the strategy instance.\n",
        "\n",
        "- *Note*: For single GPU we have `tf.distribute.OneDeviceStrategy`. This is a strategy to place all variables and computation on a single specified device.\n",
        " > This strategy is distinct from the Default Strategy in a number of ways. In the Default Strategy, the variable placement logic remains unchanged when compared to running TensorFlow without any distribution strategy. But when using OneDeviceStrategy, all variables created in its scope are explicitly placed on the specified device. Moreover, any functions called via OneDeviceStrategy.run will also be placed on the specified device. Input distributed through this strategy will be prefetched to the specified device. In the Default Strategy, there is no input distribution."
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-17T07:17:48.466256Z",
          "iopub.status.busy": "2022-04-17T07:17:48.465961Z",
          "iopub.status.idle": "2022-04-17T07:17:48.476242Z",
          "shell.execute_reply": "2022-04-17T07:17:48.475399Z",
          "shell.execute_reply.started": "2022-04-17T07:17:48.466215Z"
        },
        "id": "3e8S6jNP8dYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_strategy(xla, fp16, no_cuda):\n",
        "    print(\" Tensorflow: setting up strategy\")\n",
        "\n",
        "    # setup xla\n",
        "    if xla:\n",
        "        print(\" XLA Enabled\")\n",
        "        tf.config.optimizer.set_jit(True)\n",
        "\n",
        "    # setup mixed precision training\n",
        "    if fp16:\n",
        "        # Set to float16 at first\n",
        "        print(\" Mixed Precision Training Enabled\")\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
        "        tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "\n",
        "    # setup distribution strategy\n",
        "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "    if no_cuda:\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "    else:\n",
        "        if len(gpus) == 0:\n",
        "            print(\" One Device Strategy [CPU] Enabled\")\n",
        "            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "        elif len(gpus) == 1:\n",
        "            print(\" One Device Strategy [GPU] Enabled\")\n",
        "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
        "        elif len(gpus) > 1:\n",
        "            print(\" Mirrored Strategy Enabled\")\n",
        "            # If only want to use a specific subset of GPUs use CUDA_VISIBLE_DEVICES=0`\n",
        "            strategy = tf.distribute.MirroredStrategy()\n",
        "        else:\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "\n",
        "    return strategy\n",
        "\n",
        "def n_replicas(strategy):\n",
        "    # return number of devices\n",
        "    return strategy.num_replicas_in_sync\n",
        "\n",
        "# note:\n",
        "# huggingface TF-T5 implementation has issues when mixed precision is enabled\n",
        "# we will disable FP16 for this but can be used for training any other model\n",
        "strategy = setup_strategy(xla=True, fp16=False, no_cuda=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-21T12:13:24.841236Z",
          "iopub.status.busy": "2022-04-21T12:13:24.840884Z",
          "iopub.status.idle": "2022-04-21T12:13:24.853474Z",
          "shell.execute_reply": "2022-04-21T12:13:24.852737Z",
          "shell.execute_reply.started": "2022-04-21T12:13:24.841197Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBWLrHLP8dYT",
        "outputId": "a9d20961-35bb-4768-beb5-0e5466bd267e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Tensorflow: setting up strategy\n",
            " XLA Enabled\n",
            " One Device Strategy [GPU] Enabled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section6\"><font color='#FF6F00'><h2>MBPP (Mostly Basic Python Problems) Benchmark</h2></font></a>\n",
        "\n",
        "- The Mostly Basic Programming Problems dataset was introduced in [Program Synthesis with Large Language Models](https://arxiv.org/pdf/2108.07732.pdf). It contains 974 short Python functions designed to be solved by entry-level programmers, text descriptions of those programs, and test cases to check for functional correctness. This dataset consists of a large set of crowd-sourced questions and a smaller set of questions edited and hand verified by the authors.\n",
        "\n",
        "<br>\n",
        "<center><img src=\"https://media.arxiv-vanity.com/render-output/5567015/x1.png\" alt=\"mbpp\" width=\"400\"/></center>\n",
        "<br>\n",
        "\n",
        "- The problems range from simple numeric manipulations or tasks that require basic usage of standard library functions to tasks that require nontrivial external knowledge, such as the definition of particular notable integer sequences.\n",
        "\n",
        "- Given this, they manually inspected, edited, and pruned a subset of the questions, yielding 426 hand-verified questions, which we refer to as the edited dataset. For each question in the edited dataset, they ensured it had a standard Python function signature, that it was unambiguous to a human, and that its test cases accurately reflected the text description.\n",
        "\n",
        "- The original and cleaned dataset is available [here](https://github.com/google-research/google-research/tree/master/mbpp).\n",
        "\n",
        "<a id=\"section7\"><font color='#FF6F00'><h2>Dataset Processing</h2></font></a>\n",
        "\n",
        "Below is the complete process from data downloading to processing and converting to TensorFlow `tf.data.Dataset` object. It gives an overview of our data loading pipeline but to have a comprehensive idea one can read about tf.Data API [here](https://www.tensorflow.org/guide/data) and how to optimize data pipeline performance [here](https://www.tensorflow.org/guide/data_performance)\n",
        "\n",
        "*Note: check `run()` method for complete code flow.*\n",
        "\n",
        "- `download_dataset()` downloads the mbpp dataset from url and we load using hf datasets `load_dataset()` function.\n",
        "- `convert_examples_to_features()` creates features (input_ids, attention_mask, labels) required for our model training using `map()` function.\n",
        "- split the training data into train/test with test_size of size 0.1% of total training data using `split()` method.\n",
        "- `get_train_tfdataset()` and `get_validation_tfdataset()`converts the dataset to tensorflow tf.data.Dataset object.\n",
        "\n",
        "    - The `Dataset.from_generator` constructor converts the python generator to a fully functional `tf.data.Dataset`. The constructor takes a callable as input, not an iterator. This allows it to restart the generator when it reaches the end. The `output_types` argument is required because `tf.data` builds a `tf.Graph` internally, and graph edges require a `tf.dtype`. It's also important to note that the `output_shapes` and `output_types` follow the same nesting rules as other dataset methods.\n",
        "    \n",
        "    - The simplest way to iterate over a dataset in multiple epochs is to use the `Dataset.repeat()` transformation. Applying the `Dataset.repeat()` transformation with no arguments will repeat the input indefinitely.\n",
        "    - The `Dataset.shuffle()` transformation maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer. We apply `Dataset.shuffle()` only for training dataset and not for validation dataset.\n",
        "    - A `Dataset.batch` applied after `Dataset.repeat` will yield batches that straddle epoch boundaries. If you need clear epoch separation, put `Dataset.batch` before the repeat.\n",
        "    - Prefetching overlaps the preprocessing and model execution of a training step. While the model is executing training step `s`, the input pipeline is reading the data for step `s+1`. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data. The `tf.data` API provides the `tf.data.Dataset.prefetch` transformation. It can be used to decouple the time when data is produced from the time when data is consumed. The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step. You could either manually tune this value, or set it to `tf.data.AUTOTUNE`, which will prompt the `tf.data` runtime to tune the value dynamically at runtime."
      ],
      "metadata": {
        "id": "wDscpqhj8dYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset(cache_dir):\n",
        "    # download data using a keras utility\n",
        "    _url = \"https://raw.githubusercontent.com/google-research/google-research/master/mbpp/mbpp.jsonl\" # download mbpp dataset\n",
        "    dataset_path = tf.keras.utils.get_file(\"mbpp.jsonl\", origin=_url, cache_dir=cache_dir, cache_subdir=cache_dir)\n",
        "    return dataset_path\n",
        "\n",
        "def convert_examples_to_features(examples, tokenizer, args):\n",
        "    # encode text-code pairs\n",
        "    texts = examples['text']\n",
        "    codes = examples['code']\n",
        "    # tests = [\" \".join(test) for test in examples['test_list']] # convert list of test cases to single string\n",
        "\n",
        "    # encode texts by prepending the task for input sequence\n",
        "    inputs = [args.prefix + text for text in texts]\n",
        "    model_inputs = tokenizer(inputs, max_length=args.max_input_length, padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # encode texts by prepending the task for input sequence and appending the test sequence\n",
        "    # inputs = [args.prefix + text + \" \" + test for text, test in zip(texts, tests)]\n",
        "    # model_inputs = tokenizer(inputs, max_length=args.max_input_length, padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # encode texts by prepending the task for input sequence\n",
        "    labels = tokenizer(codes, max_length=args.max_target_length, padding=\"max_length\", truncation=True).input_ids\n",
        "\n",
        "    # we need to replace the index of the padding tokens by -100\n",
        "    # such that they are not taken into account by the CrossEntropyLoss\n",
        "    labels_with_ignore_index = []\n",
        "    for labels_example in labels:\n",
        "        labels_example = [label if label != 0 else -100 for label in labels_example]\n",
        "        labels_with_ignore_index.append(labels_example)\n",
        "    model_inputs[\"labels\"] = labels_with_ignore_index\n",
        "\n",
        "    # return features\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def get_train_tfdataset(train_dataset, num_train_examples, args):\n",
        "    # select feature columns\n",
        "    columns = ['input_ids', 'attention_mask', 'labels']\n",
        "    # set to tensorflow format\n",
        "    train_dataset.set_format(type='tensorflow', columns=columns)\n",
        "\n",
        "    # specify return types\n",
        "    return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, 'labels':tf.int32}\n",
        "    # specify return shapes\n",
        "    return_shapes = {'input_ids': tf.TensorShape([None]),'attention_mask': tf.TensorShape([None]), 'labels': tf.TensorShape([None])}\n",
        "    # initialize dataset\n",
        "    tf_dataset = tf.data.Dataset.from_generator(lambda : train_dataset, return_types, return_shapes)\n",
        "\n",
        "    # turn off auto-sharding\n",
        "    options = tf.data.Options()\n",
        "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
        "    tf_dataset = tf_dataset.with_options(options)\n",
        "\n",
        "    # repeat, shuffle, batch, prefetch\n",
        "    ds = (\n",
        "        tf_dataset.repeat()\n",
        "        .shuffle(num_train_examples, seed=args.seed)\n",
        "        .batch(args.train_batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    # distribute dataset to devices\n",
        "    return strategy.experimental_distribute_dataset(ds)\n",
        "\n",
        "def get_validation_tfdataset(eval_dataset, num_validation_examples, args):\n",
        "    # select feature columns\n",
        "    columns = ['input_ids', 'attention_mask', 'labels']\n",
        "    # set to tensorflow format\n",
        "    eval_dataset.set_format(type='tensorflow', columns=columns)\n",
        "\n",
        "    # specify return types\n",
        "    return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, 'labels':tf.int32}\n",
        "    # specify return shapes\n",
        "    return_shapes = {'input_ids': tf.TensorShape([None]),'attention_mask': tf.TensorShape([None]), 'labels': tf.TensorShape([None])}\n",
        "    # initialize dataset\n",
        "    tf_dataset = tf.data.Dataset.from_generator(lambda : eval_dataset, return_types, return_shapes)\n",
        "\n",
        "    # turn off auto-sharding\n",
        "    options = tf.data.Options()\n",
        "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
        "    tf_dataset = tf_dataset.with_options(options)\n",
        "\n",
        "    # repeat, batch, prefetch\n",
        "    ds = (\n",
        "        tf_dataset.repeat()\n",
        "        .batch(args.validation_batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    # distribute dataset to devices\n",
        "    return strategy.experimental_distribute_dataset(ds)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-21T12:13:24.856443Z",
          "iopub.status.busy": "2022-04-21T12:13:24.855918Z",
          "iopub.status.idle": "2022-04-21T12:13:24.874475Z",
          "shell.execute_reply": "2022-04-21T12:13:24.873654Z",
          "shell.execute_reply.started": "2022-04-21T12:13:24.856404Z"
        },
        "id": "n9d-IMXL8dYU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section8\"><font color='#FF6F00'><h2>Utility Functions / Class</h2></font></a>\n",
        "\n",
        "- *fix_all_seeds()* - sets the random seed for deterministic results.\n",
        "- *init_logger()* - initialize logger for tracking events.\n",
        "- *ProgressBar()* - custom progress bar to display metrics."
      ],
      "metadata": {
        "id": "hTWgywNe8dYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_all_seeds(seed):\n",
        "    # set random seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "def init_logger(log_file=None, log_file_level=logging.NOTSET):\n",
        "    # initialize logger for tracking events and save in file\n",
        "    if isinstance(log_file, Path):\n",
        "        log_file = str(log_file)\n",
        "    log_format = logging.Formatter(\n",
        "        fmt='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "        datefmt='%m/%d/%Y %H:%M:%S'\n",
        "    )\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.INFO)\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setFormatter(log_format)\n",
        "    logger.handlers = [console_handler]\n",
        "    if log_file and log_file != '':\n",
        "        file_handler = logging.FileHandler(log_file)\n",
        "        file_handler.setLevel(log_file_level)\n",
        "        # file_handler.setFormatter(log_format)\n",
        "        logger.addHandler(file_handler)\n",
        "    return logger\n",
        "\n",
        "class ProgressBar(object):\n",
        "    # custom progress bar\n",
        "    def __init__(self, n_total,width=30,desc = 'Training'):\n",
        "        self.width = width\n",
        "        self.n_total = n_total\n",
        "        self.start_time = time.time()\n",
        "        self.desc = desc\n",
        "\n",
        "    def __call__(self, step, info={}):\n",
        "        now = time.time()\n",
        "        current = step + 1\n",
        "        recv_per = current / self.n_total\n",
        "        bar = f'[{self.desc}] {current}/{self.n_total} ['\n",
        "        if recv_per >= 1:\n",
        "            recv_per = 1\n",
        "        prog_width = int(self.width * recv_per)\n",
        "        if prog_width > 0:\n",
        "            bar += '=' * (prog_width - 1)\n",
        "            if current< self.n_total:\n",
        "                bar += \">\"\n",
        "            else:\n",
        "                bar += '='\n",
        "        bar += '.' * (self.width - prog_width)\n",
        "        bar += ']'\n",
        "        show_bar = f\"\\r{bar}\"\n",
        "        time_per_unit = (now - self.start_time) / current\n",
        "        if current < self.n_total:\n",
        "            eta = time_per_unit * (self.n_total - current)\n",
        "            if eta > 3600:\n",
        "                eta_format = ('%d:%02d:%02d' %\n",
        "                              (eta // 3600, (eta % 3600) // 60, eta % 60))\n",
        "            elif eta > 60:\n",
        "                eta_format = '%d:%02d' % (eta // 60, eta % 60)\n",
        "            else:\n",
        "                eta_format = '%ds' % eta\n",
        "            time_info = f' - ETA: {eta_format}'\n",
        "        else:\n",
        "            if time_per_unit >= 1:\n",
        "                time_info = f' {time_per_unit:.1f}s/step'\n",
        "            elif time_per_unit >= 1e-3:\n",
        "                time_info = f' {time_per_unit * 1e3:.1f}ms/step'\n",
        "            else:\n",
        "                time_info = f' {time_per_unit * 1e6:.1f}us/step'\n",
        "\n",
        "        show_bar += time_info\n",
        "        if len(info) != 0:\n",
        "            show_info = f'{show_bar} ' + \\\n",
        "                        \"-\".join([f' {key}: {value:.4f} ' if key != \"learning_rate\" else f' {key}: {value:.8f} ' for key, value in info.items()])\n",
        "            print(show_info, end='')\n",
        "        else:\n",
        "            print(show_bar, end='')"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-21T12:13:24.876245Z",
          "iopub.status.busy": "2022-04-21T12:13:24.875766Z",
          "iopub.status.idle": "2022-04-21T12:13:24.89338Z",
          "shell.execute_reply": "2022-04-21T12:13:24.892695Z",
          "shell.execute_reply.started": "2022-04-21T12:13:24.876208Z"
        },
        "id": "89oI6v-H8dYU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section9\"><font color='#FF6F00'><h2>Custom Training Loop</h2></font></a>\n",
        "\n",
        "Here we define our Trainer class for training and evaluation loop. Custom training loops provide flexibility and a greater control on training. They also make it is easier to debug the model and the training loop. The following code block sets up these training steps:\n",
        "\n",
        "1. Iterate each epoch. An epoch is one pass through the dataset.\n",
        "2. Within an epoch, iterate over each example in the training Dataset grabbing its features (x) and label (y).\n",
        "3. Using the features, make a prediction and compare it with the label. Measure the inaccuracy of the prediction and use that to calculate the model's loss and gradients.\n",
        "4. Use an optimizer to update the model's parameters.\n",
        "5. Keep track of some stats for logging.\n",
        "6. Repeat for each epoch.\n",
        "\n",
        "Below figure demonstrates the code flow of our `trainer.train()` method. One can read about Custom Training Loops [here](https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough) and [here](https://www.tensorflow.org/tutorials/distribute/custom_training).\n",
        "\n",
        "<br>\n",
        "<center><img src=\"tf_process.png\" alt=\"custom_training_loop\" width=\"550\" height=\"500\"/></center>\n",
        "<br>\n",
        "\n",
        "*Notes:*\n",
        "- Iterate over the `train_dist_dataset` and `test_dist_dataset` using a for `x in ...` construct.\n",
        "\n",
        "- model, optimizer, and checkpoint must be created under `strategy.scope`.\n",
        "\n",
        "- `strategy.run()` replicates the provided computation and runs it with the distributed input. It returns results from each local replica in the strategy, and there are multiple ways to consume this result.\n",
        "\n",
        "- A model checkpointed with a `tf.distribute.Strategy` can be restored with or without a strategy.\n",
        "\n",
        "- `tf.function` helps to make graphs out of your programs. It is a transformation tool that creates Python-independent dataflow graphs out of your Python code. This will help you create performant and portable models, and it is required to use SavedModel. One can read more about `tf.function` [here](https://www.tensorflow.org/guide/function).\n",
        "\n",
        "- Using `tf.reduce_mean` is not recommended. Doing so divides the loss by actual per replica batch size which may vary step to step."
      ],
      "metadata": {
        "id": "6t5IUJ9z8dYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self, model, args, train_dataset, validation_dataset,\n",
        "        num_train_examples, num_validation_examples\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.args = args\n",
        "\n",
        "        self.train_dataset = train_dataset\n",
        "        self.num_train_examples = num_train_examples\n",
        "\n",
        "        self.validation_dataset = validation_dataset\n",
        "        self.num_validation_examples = num_validation_examples\n",
        "\n",
        "        self.global_step = 0\n",
        "        self.eval_loss = tf.keras.metrics.Sum()\n",
        "\n",
        "    def create_optimizer_and_scheduler(self, num_training_steps):\n",
        "        # creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay.\n",
        "        num_warmup_steps = math.ceil(num_training_steps * self.args.warmup_ratio)\n",
        "        self.optimizer, self.lr_scheduler = transformers.create_optimizer(\n",
        "            init_lr=self.args.learning_rate,\n",
        "            num_train_steps=num_training_steps,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            weight_decay_rate=self.args.weight_decay,\n",
        "            adam_epsilon=self.args.adam_epsilon\n",
        "        )\n",
        "\n",
        "    def evaluation_step(self, features, labels, nb_instances_in_global_batch):\n",
        "        # forward pass\n",
        "        outputs = self.model(input_ids=features['input_ids'], attention_mask=features['attention_mask'], labels=labels, training=False)[:2]\n",
        "        loss, logits = outputs[:2]\n",
        "        # loss scaling\n",
        "        scaled_loss = loss / tf.cast(nb_instances_in_global_batch, dtype=loss.dtype)\n",
        "        # add current batch loss\n",
        "        self.eval_loss.update_state(scaled_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def distributed_evaluation_steps(self, batch):\n",
        "        features = {k: v for k, v in batch.items() if 'labels' not in k}\n",
        "        labels = batch['labels']\n",
        "        nb_instances = tf.reduce_sum(tf.cast(labels != -100, dtype=tf.int32))\n",
        "        # strategy.run() expects args to be a list or tuple\n",
        "        inputs = (features, labels, nb_instances)\n",
        "        # `run` replicates the provided computation and runs with the distributed input\n",
        "        strategy.run(self.evaluation_step, inputs)\n",
        "\n",
        "    def evaluate(self):\n",
        "        # calculate total validation steps\n",
        "        steps = math.ceil(self.num_validation_examples / self.args.validation_batch_size)\n",
        "        # reset eval loss after every epoch\n",
        "        self.eval_loss.reset_states()\n",
        "        logs = {}\n",
        "        pbar = ProgressBar(n_total=steps, desc='Evaluating')\n",
        "        # iterate over validation dataset\n",
        "        for step, batch in enumerate(self.validation_dataset):\n",
        "            # distributed evaluation step\n",
        "            self.distributed_evaluation_steps(batch)\n",
        "            logs[\"eval_loss\"] = self.eval_loss.result() / (step + 1)\n",
        "            pbar(step=step, info=logs)\n",
        "            if step == steps - 1:\n",
        "                break\n",
        "        print(\"\\n------------- validation result -----------------\")\n",
        "\n",
        "    def apply_gradients(self, features, labels, nb_instances_in_global_batch):\n",
        "        # forward pass\n",
        "        outputs = self.model(input_ids=features['input_ids'], attention_mask=features['attention_mask'], labels=labels, training=True)[:2]\n",
        "        loss, logits = outputs[:2]\n",
        "        # loss scaling\n",
        "        scaled_loss = loss / tf.cast(nb_instances_in_global_batch, dtype=loss.dtype)\n",
        "        # calculate gradients\n",
        "        gradients = tf.gradients(scaled_loss, self.model.trainable_variables)\n",
        "        # convert gradients with nan value\n",
        "        gradients = [g if g is not None else tf.zeros_like(v) for g, v in zip(gradients, self.model.trainable_variables)]\n",
        "        # optimize the model\n",
        "        self.optimizer.apply_gradients(list(zip(gradients, self.model.trainable_variables)))\n",
        "        # add current batch loss\n",
        "        self.train_loss.update_state(scaled_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def distributed_training_steps(self, batch):\n",
        "        with strategy.scope():\n",
        "            features = {k: v for k, v in batch.items() if 'labels' not in k}\n",
        "            labels = batch['labels']\n",
        "            nb_instances = tf.reduce_sum(tf.cast(labels != -100, dtype=tf.int32))\n",
        "            # strategy.run() expects args to be a list or tuple\n",
        "            inputs = (features, labels, nb_instances)\n",
        "            # `run` replicates the provided computation and runs with the distributed input.\n",
        "            strategy.run(self.apply_gradients, inputs)\n",
        "\n",
        "    def train(self):\n",
        "        # calculate total training steps\n",
        "        num_updates_per_epoch = self.num_train_examples // args.train_batch_size\n",
        "        self.steps_per_epoch = num_updates_per_epoch\n",
        "        t_total = self.steps_per_epoch * self.args.epochs\n",
        "\n",
        "        with strategy.scope():\n",
        "            # optimizer, and checkpoint must be created under `strategy.scope`\n",
        "            # create optimizer and scheduler\n",
        "            self.create_optimizer_and_scheduler(num_training_steps=t_total)\n",
        "\n",
        "            # create checkpoint manager\n",
        "            folder = os.path.join(self.args.output_dir, self.args.checkpoint_dir)\n",
        "            ckpt = tf.train.Checkpoint(optimizer=self.optimizer, model=self.model)\n",
        "            self.model.ckpt_manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=1)\n",
        "            iterations = self.optimizer.iterations\n",
        "\n",
        "            logger.info(\"***** Running training *****\")\n",
        "            logger.info(f\"  Num examples = {self.num_train_examples}\")\n",
        "            logger.info(f\"  Num Epochs = {self.args.epochs}\")\n",
        "            logger.info(f\"  Total train batch size (w. parallel & distributed) = {self.args.train_batch_size * n_replicas(strategy)}\")\n",
        "            logger.info(f\"  Steps per epoch = {self.steps_per_epoch}\")\n",
        "            logger.info(f\"  Total optimization steps = {t_total}\")\n",
        "\n",
        "            self.train_loss = tf.keras.metrics.Sum(name=\"training_loss\")\n",
        "            start_time = datetime.datetime.now()\n",
        "            for epoch_iter in range(self.args.epochs):\n",
        "                # training loop\n",
        "                logger.info(f\"Epoch {epoch_iter + 1}/{self.args.epochs}\")\n",
        "\n",
        "                pbar = ProgressBar(n_total=self.steps_per_epoch, desc='Training')\n",
        "                # iterate over training dataset\n",
        "                for step, batch in enumerate(self.train_dataset):\n",
        "                    # distributed training step\n",
        "                    self.distributed_training_steps(batch)\n",
        "\n",
        "                    self.global_step = iterations.numpy()\n",
        "                    training_loss = self.train_loss.result() / (step + 1)\n",
        "\n",
        "                    logs = {}\n",
        "                    logs[\"training_loss\"] = training_loss.numpy()\n",
        "                    logs[\"learning_rate\"] = self.lr_scheduler(self.global_step).numpy()\n",
        "                    pbar(step=step, info=logs)\n",
        "\n",
        "                    if self.global_step % self.steps_per_epoch == 0:\n",
        "                        print(\"\\n------------- train result -----------------\")\n",
        "                        # call to evaluation loop\n",
        "                        self.evaluate()\n",
        "                        # save checkpoint\n",
        "                        ckpt_save_path = self.model.ckpt_manager.save()\n",
        "                        logger.info(f\"Saving checkpoint at {ckpt_save_path}\")\n",
        "                        break\n",
        "\n",
        "                # reset train loss after every epoch\n",
        "                self.train_loss.reset_states()\n",
        "            end_time = datetime.datetime.now()\n",
        "            logger.info(f\"Training took: {str(end_time - start_time)}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-21T12:13:24.896013Z",
          "iopub.status.busy": "2022-04-21T12:13:24.895769Z",
          "iopub.status.idle": "2022-04-21T12:13:24.927801Z",
          "shell.execute_reply": "2022-04-21T12:13:24.927038Z",
          "shell.execute_reply.started": "2022-04-21T12:13:24.895982Z"
        },
        "id": "WuXonSlc8dYV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section10\"><font color='#FF6F00'><h2>Run</h2></font></a>\n",
        "\n",
        "The `run()` function defines our execution process. We download, load and preprocess and convert our data into `tf.data.Dataset` format. We initialize tokenizer and model. The model needs to be created under `strategy.scope()`. We create instance of our `Trainer` and pass everything to `.train()` method for running our custom training loop. In the end we save our model and tokenizer using `.save_pretrained()` method."
      ],
      "metadata": {
        "id": "_9Sb8gGs8dYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(args):\n",
        "    logger.info(\" Starting training / evaluation\")\n",
        "\n",
        "    logger.info(\" Downloading Data Files\")\n",
        "    dataset_path = download_dataset(args.cache_dir)\n",
        "\n",
        "    logger.info(\" Loading Data Files\")\n",
        "    dataset = load_dataset('json', data_files=dataset_path)\n",
        "    # train test split\n",
        "    dataset = dataset['train'].train_test_split(0.1, shuffle=False)\n",
        "\n",
        "    logger.info(\" Initializing Tokenizer\")\n",
        "    tokenizer = transformers.RobertaTokenizer.from_pretrained(args.tokenizer_name)\n",
        "\n",
        "    logger.info(\" Preparing Features\")\n",
        "    dataset = dataset.map(convert_examples_to_features, batched=True, fn_kwargs={\"tokenizer\":tokenizer, \"args\":args})\n",
        "\n",
        "    logger.info(\" Intializing training and validation dataset \")\n",
        "    train_dataset = dataset['train']\n",
        "    num_train_examples = len(dataset['train'])\n",
        "    # create tf train dataset\n",
        "    tf_train_dataset = get_train_tfdataset(train_dataset, num_train_examples, args)\n",
        "\n",
        "    validation_dataset = dataset['test']\n",
        "    num_validation_examples = len(dataset['test'])\n",
        "    # create tf validation dataset\n",
        "    tf_validation_dataset = get_validation_tfdataset(train_dataset, num_validation_examples, args)\n",
        "\n",
        "    logger.info(f' Intializing model | {args.model_type.upper()} ')\n",
        "    with strategy.scope():\n",
        "        # model must be created under `strategy.scope`\n",
        "        model = transformers.TFT5ForConditionalGeneration.from_pretrained(args.model_name_or_path, from_pt=True)\n",
        "\n",
        "    # custom training loop\n",
        "    trainer = Trainer(model, args, tf_train_dataset, tf_validation_dataset, num_train_examples, num_validation_examples)\n",
        "    trainer.train()\n",
        "\n",
        "    # save pretrained model and tokenizer\n",
        "    logger.info(f\" Saving model in {args.save_dir}\")\n",
        "    trainer.model.save_pretrained(args.save_dir)\n",
        "    tokenizer.save_pretrained(args.save_dir)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-21T12:13:24.929815Z",
          "iopub.status.busy": "2022-04-21T12:13:24.929029Z",
          "iopub.status.idle": "2022-04-21T12:13:24.93972Z",
          "shell.execute_reply": "2022-04-21T12:13:24.939029Z",
          "shell.execute_reply.started": "2022-04-21T12:13:24.92978Z"
        },
        "id": "gjygMJh18dYV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section11\"><font color='#FF6F00'><h2>Execute</h2></font></a>\n",
        "\n",
        "Below we define our training arguments - model, data, optimizer, training and initialize directories. Initialize logger for logging and tracking metrics. We call `fix_all_seeds()` to set the global seed. Then finally we execute our `run()` method by passing our training `args`."
      ],
      "metadata": {
        "id": "j6N7z7zx8dYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    # define training arguments\n",
        "\n",
        "    # MODEL\n",
        "    model_type = 't5'\n",
        "    tokenizer_name = 'Salesforce/codet5-base'\n",
        "    model_name_or_path = 'Salesforce/codet5-base'\n",
        "\n",
        "    # DATA\n",
        "    train_batch_size = 8\n",
        "    validation_batch_size = 8\n",
        "    max_input_length = 48\n",
        "    max_target_length = 128\n",
        "    prefix = \"Generate Python: \"\n",
        "\n",
        "    # OPTIMIZER\n",
        "    learning_rate = 3e-4\n",
        "    weight_decay = 1e-4\n",
        "    warmup_ratio = 0.2\n",
        "    adam_epsilon = 1e-8\n",
        "\n",
        "    # TRAINING\n",
        "    seed = 2022\n",
        "    epochs = 20\n",
        "\n",
        "    # DIRECTORIES\n",
        "    output_dir = \"runs/\"\n",
        "    logging_dir = f\"{output_dir}/logs/\"\n",
        "    checkpoint_dir = f\"checkpoint\"\n",
        "    save_dir = f\"{output_dir}/saved_model/\"\n",
        "    cache_dir = '../working/'\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    Path(logging_dir).mkdir(parents=True, exist_ok=True)\n",
        "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# initialize training arguments\n",
        "args = Args()\n",
        "# initialize logger\n",
        "logger = init_logger(log_file=os.path.join(args.logging_dir, f\"{args.model_type}-{time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime())}.log\"))\n",
        "# fix all seeds\n",
        "fix_all_seeds(args.seed)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # run training and evaluation\n",
        "    dataset = run(args)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2022-04-21T12:13:24.941656Z",
          "iopub.status.busy": "2022-04-21T12:13:24.940992Z",
          "iopub.status.idle": "2022-04-21T12:34:12.757886Z",
          "shell.execute_reply": "2022-04-21T12:34:12.755743Z",
          "shell.execute_reply.started": "2022-04-21T12:13:24.941619Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-bKgLhd8dYW",
        "outputId": "73bb8cf1-262d-46e5-d593-709c747d1897"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:28:04 - INFO - root -    Starting training / evaluation\n",
            "11/06/2023 23:28:04 - INFO - root -    Downloading Data Files\n",
            "11/06/2023 23:28:04 - INFO - root -    Loading Data Files\n",
            "11/06/2023 23:28:05 - INFO - root -    Initializing Tokenizer\n",
            "11/06/2023 23:28:05 - INFO - root -    Preparing Features\n",
            "11/06/2023 23:28:06 - INFO - root -    Intializing training and validation dataset \n",
            "11/06/2023 23:28:06 - INFO - root -    Intializing model | T5 \n",
            "11/06/2023 23:28:09 - INFO - root -   ***** Running training *****\n",
            "11/06/2023 23:28:09 - INFO - root -     Num examples = 876\n",
            "11/06/2023 23:28:09 - INFO - root -     Num Epochs = 20\n",
            "11/06/2023 23:28:09 - INFO - root -     Total train batch size (w. parallel & distributed) = 8\n",
            "11/06/2023 23:28:09 - INFO - root -     Steps per epoch = 109\n",
            "11/06/2023 23:28:09 - INFO - root -     Total optimization steps = 2180\n",
            "11/06/2023 23:28:09 - INFO - root -   Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 948.5ms/step  training_loss: 0.0072 - learning_rate: 0.00007500 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 772.4ms/step  eval_loss: 0.0030 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:30:25 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-1\n",
            "11/06/2023 23:30:25 - INFO - root -   Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 363.5ms/step  training_loss: 0.0029 - learning_rate: 0.00015000 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 127.7ms/step  eval_loss: 0.0021 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:31:26 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-2\n",
            "11/06/2023 23:31:47 - INFO - root -   Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 374.0ms/step  training_loss: 0.0023 - learning_rate: 0.00022500 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 117.6ms/step  eval_loss: 0.0016 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:32:41 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-3\n",
            "11/06/2023 23:32:41 - INFO - root -   Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 380.9ms/step  training_loss: 0.0020 - learning_rate: 0.00030000 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 121.3ms/step  eval_loss: 0.0013 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:33:40 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-4\n",
            "11/06/2023 23:33:40 - INFO - root -   Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 373.2ms/step  training_loss: 0.0016 - learning_rate: 0.00028125 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 118.2ms/step  eval_loss: 0.0010 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:34:52 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-5\n",
            "11/06/2023 23:34:52 - INFO - root -   Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 375.2ms/step  training_loss: 0.0012 - learning_rate: 0.00026250 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 117.0ms/step  eval_loss: 0.0007 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:36:34 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-6\n",
            "11/06/2023 23:37:14 - INFO - root -   Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 371.7ms/step  training_loss: 0.0010 - learning_rate: 0.00024375 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 117.9ms/step  eval_loss: 0.0005 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:38:44 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-7\n",
            "11/06/2023 23:39:36 - INFO - root -   Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 371.2ms/step  training_loss: 0.0008 - learning_rate: 0.00022500 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 126.7ms/step  eval_loss: 0.0003 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:41:14 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-8\n",
            "11/06/2023 23:41:14 - INFO - root -   Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 370.7ms/step  training_loss: 0.0006 - learning_rate: 0.00020625 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 117.0ms/step  eval_loss: 0.0003 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:42:45 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-9\n",
            "11/06/2023 23:42:45 - INFO - root -   Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 371.0ms/step  training_loss: 0.0005 - learning_rate: 0.00018750 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 118.3ms/step  eval_loss: 0.0002 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:44:18 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-10\n",
            "11/06/2023 23:44:18 - INFO - root -   Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 374.5ms/step  training_loss: 0.0004 - learning_rate: 0.00016875 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 117.9ms/step  eval_loss: 0.0001 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:45:54 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-11\n",
            "11/06/2023 23:46:40 - INFO - root -   Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 363.5ms/step  training_loss: 0.0003 - learning_rate: 0.00015000 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 117.2ms/step  eval_loss: 0.0001 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:48:16 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-12\n",
            "11/06/2023 23:48:16 - INFO - root -   Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 362.9ms/step  training_loss: 0.0003 - learning_rate: 0.00013125 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 121.4ms/step  eval_loss: 0.0001 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:49:38 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-13\n",
            "11/06/2023 23:49:38 - INFO - root -   Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 365.7ms/step  training_loss: 0.0002 - learning_rate: 0.00011250 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 122.7ms/step  eval_loss: 0.0000 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:51:12 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-14\n",
            "11/06/2023 23:51:12 - INFO - root -   Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 362.7ms/step  training_loss: 0.0002 - learning_rate: 0.00009375 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 116.4ms/step  eval_loss: 0.0000 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:52:40 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-15\n",
            "11/06/2023 23:53:34 - INFO - root -   Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 365.8ms/step  training_loss: 0.0001 - learning_rate: 0.00007500 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 120.7ms/step  eval_loss: 0.0000 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:55:08 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-16\n",
            "11/06/2023 23:55:08 - INFO - root -   Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 359.9ms/step  training_loss: 0.0001 - learning_rate: 0.00005625 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 121.4ms/step  eval_loss: 0.0000 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:56:34 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-17\n",
            "11/06/2023 23:57:30 - INFO - root -   Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 372.3ms/step  training_loss: 0.0001 - learning_rate: 0.00003750 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 127.4ms/step  eval_loss: 0.0000 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/06/2023 23:59:11 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-18\n",
            "11/06/2023 23:59:52 - INFO - root -   Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 354.7ms/step  training_loss: 0.0001 - learning_rate: 0.00001875 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 112.5ms/step  eval_loss: 0.0000 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/07/2023 00:01:00 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-19\n",
            "11/07/2023 00:01:00 - INFO - root -   Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 372.1ms/step  training_loss: 0.0001 - learning_rate: 0.00000000 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 120.0ms/step  eval_loss: 0.0000 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/07/2023 00:02:29 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-20\n",
            "11/07/2023 00:02:29 - INFO - root -   Training took: 0:34:19.686333\n",
            "11/07/2023 00:02:29 - INFO - root -    Saving model in runs//saved_model/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "8OfHVaTRF0X3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.make_archive(\"runs.zip\", \"zip\", \"runs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "unOgkkhTGFow",
        "outputId": "7b30ab28-f6cb-40b8-c41c-07ae0f8e4520"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/runs.zip.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section12\"><font color='#FF6F00'><h2>Predict</h2></font></a>\n",
        "\n",
        "- The `predict_from_dataset()` and `predict_from_text()` method is where we do our predictions/inference. Here we are using the texts in test set to generate our code in `predict_from_dataset()` but one can call `predict_from_text()` and provide custom input.\n",
        "\n",
        "- `run_predict()` is the main function which calls `model.generate()` method to output the model predictions using decoding technique.\n",
        "\n",
        "- In `predict_from_dataset()` we randomly choose an index from the test dataset, so everytime the function is called we randomly sample an index from the dataset.\n",
        "\n",
        "- Notes:\n",
        "    -  Here we are using Top-p (nucleus) sampling technique for decoding. In simple, top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. This way, the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution. Here we can use Top-p sampling by setting 0 < `top_p` < 1.\n",
        "\n",
        "    - `Top-p` can also be used in combination with `Top-K`, which can avoid very low ranked words while allowing for some dynamic selection.\n",
        "\n",
        "    - `Reptition_penalty` can be used to penalize words that were already generated or belong to the context.\n",
        "\n",
        "    - To get multiple independently sampled outputs, we can again set the parameter `num_return_sequences` > 1."
      ],
      "metadata": {
        "id": "GYZMlOUh8dYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_predict(args, text):\n",
        "    # load saved finetuned model\n",
        "    model = transformers.TFT5ForConditionalGeneration.from_pretrained(args.save_dir)\n",
        "    # load saved tokenizer\n",
        "    tokenizer = transformers.RobertaTokenizer.from_pretrained(args.save_dir)\n",
        "\n",
        "     # encode texts by prepending the task for input sequence and appending the test sequence\n",
        "    query = args.prefix + text\n",
        "    encoded_text = tokenizer(query, return_tensors='tf', padding='max_length', truncation=True, max_length=args.max_input_length)\n",
        "\n",
        "    # inference\n",
        "    generated_code = model.generate(\n",
        "        encoded_text[\"input_ids\"], attention_mask=encoded_text[\"attention_mask\"],\n",
        "        max_length=args.max_target_length, top_p=0.95, top_k=50, repetition_penalty=2.0, num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    # decode generated tokens\n",
        "    decoded_code = tokenizer.decode(generated_code.numpy()[0], skip_special_tokens=True)\n",
        "    return decoded_code\n",
        "\n",
        "def predict_from_dataset(args):\n",
        "    # load using hf datasets\n",
        "    dataset = load_dataset('json', data_files='../working/mbpp.jsonl')\n",
        "    # train test split\n",
        "    dataset = dataset['train'].train_test_split(0.1, shuffle=False)\n",
        "    test_dataset = dataset['test']\n",
        "\n",
        "    # randomly select an index from the validation dataset\n",
        "    index = random.randint(0, len(test_dataset))\n",
        "    text = test_dataset[index]['text']\n",
        "    code = test_dataset[index]['code']\n",
        "\n",
        "    # run-predict on text\n",
        "    decoded_code = run_predict(args, text)\n",
        "\n",
        "    print(\"#\" * 25); print(\"QUERY: \", text);\n",
        "    print()\n",
        "    print('#' * 25); print(\"ORIGINAL: \"); print(\"\\n\", code);\n",
        "    print()\n",
        "    print('#' * 25); print(\"GENERATED: \"); print(\"\\n\", decoded_code);\n",
        "\n",
        "def predict_from_text(args, text):\n",
        "    # run-predict on text\n",
        "    decoded_code = run_predict(args, text)\n",
        "    print(\"#\" * 25); print(\"QUERY: \", text);\n",
        "    print()\n",
        "    print('#' * 25); print(\"GENERATED: \"); print(\"\\n\", decoded_code);"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-21T12:34:12.767978Z",
          "iopub.status.busy": "2022-04-21T12:34:12.766833Z",
          "iopub.status.idle": "2022-04-21T12:34:12.792282Z",
          "shell.execute_reply": "2022-04-21T12:34:12.791622Z",
          "shell.execute_reply.started": "2022-04-21T12:34:12.767944Z"
        },
        "id": "F8pnKPuf8dYW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section12a\"><font color='#425066'><h3>Predict from Dataset</h3></font></a>"
      ],
      "metadata": {
        "id": "2X7jSDtR8dYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example 1\n",
        "predict_from_dataset(args)\n",
        "# example 2\n",
        "predict_from_dataset(args)\n",
        "# example 3\n",
        "predict_from_dataset(args)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2022-04-21T12:34:12.795945Z",
          "iopub.status.busy": "2022-04-21T12:34:12.795592Z",
          "iopub.status.idle": "2022-04-21T12:34:43.907852Z",
          "shell.execute_reply": "2022-04-21T12:34:43.906366Z",
          "shell.execute_reply.started": "2022-04-21T12:34:12.79591Z"
        },
        "id": "zxzMUqD68dYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section12b\"><font color='#425066'><h3>Predict from Text</h3></font></a>"
      ],
      "metadata": {
        "id": "qw6V4izJ8dYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example 1\n",
        "predict_from_text(args, \"Write a function to add two random numbers\"); print()\n",
        "# example 2\n",
        "predict_from_text(args, \"Write a function to find the frequency of items in a list\"); print()\n",
        "# example 3\n",
        "predict_from_text(args, \"Write a function to concatenate two dictionary\"); print()"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2022-04-21T12:34:43.909917Z",
          "iopub.status.busy": "2022-04-21T12:34:43.909643Z",
          "iopub.status.idle": "2022-04-21T12:35:12.355678Z",
          "shell.execute_reply": "2022-04-21T12:35:12.354922Z",
          "shell.execute_reply.started": "2022-04-21T12:34:43.909881Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS_6toFa8dYX",
        "outputId": "6aca1908-a75d-499b-de5f-5a6097d66602"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#########################\n",
            "QUERY:  Write a function to add two random numbers\n",
            "\n",
            "#########################\n",
            "GENERATED: \n",
            "\n",
            " def add_random(num1, num2):\r\n",
            "    random = [0 for i in range (min(_int(),max((x) + 1)/3)] \r\n",
            "        if x % 2 == 0:  \r\n",
            "            return y    \r\n",
            "    else :\r\n",
            "          None;\n",
            "\n",
            "#########################\n",
            "QUERY:  Write a function to find the frequency of items in a list\n",
            "\n",
            "#########################\n",
            "GENERATED: \n",
            "\n",
            " import collections\n",
            "def freq_count(list1): \n",
            "  frequency=collections.Counter((x) for x in list 1 )   \n",
            "  return frequencies\n",
            "\n",
            "#########################\n",
            "QUERY:  Write a function to concatenate two dictionary\n",
            "\n",
            "#########################\n",
            "GENERATED: \n",
            "\n",
            " def concatenate_dict(d1, d2):\n",
            "    result = {k: v for k, val in zip(*map(_join__), dict.items()))} \n",
            "    returnResult\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_from_text(args, \"Write a function to concatenate two words\"); print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKSywdYeSd4e",
        "outputId": "6fa0bf59-49e7-4e7e-a835-7772533ae697"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#########################\n",
            "QUERY:  Write a function to concatenate two words\n",
            "\n",
            "#########################\n",
            "GENERATED: \n",
            "\n",
            " def concatenate_words(str1, str2):\r\n",
            "  word=re.sub(' +',' ', s) \r\n",
            "   return (word[0:len([w]),''.join((x for x in txt 1))])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_from_text(args, \"Write a function that creates cellphone numbers\"); print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX7ls5cKYvfO",
        "outputId": "da754d95-f9f1-403c-b113-007078d65b2d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#########################\n",
            "QUERY:  Write a function that creates cellphone numbers\n",
            "\n",
            "#########################\n",
            "GENERATED: \n",
            "\n",
            " def create_cellphone(number):\r\n",
            " number = (numeric)((int)(math.sqrt())*pow(\"2,\",Number))/4; \r\n",
            " return string\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section13\"><font color='#FF6F00'><h2>Ending Notes</h2></font></a>\n",
        "\n",
        "- In this notebook we explore how we can finetune a model on *Text-to-Code Generation* task using TensorFlow, HuggingFace and MBPP dataset.\n",
        "- The results aren't as amazing but that's because the model is t5-base and because the fine-tuning set is extremely small. Also, a quote from the paper says,\n",
        "  > While inspecting the dataset, we observed that some questions used uncommon function signatures (such as passing in\n",
        "a list and its length as two separate arguments to a function), lacked detail, were somewhat ambiguous (e.g., ‚ÄúWrite a\n",
        "python function to count the number of squares in a rectangle.‚Äù), or performed unexpected operations in a function\n",
        "that were paired with the provided tests (e.g., casting a float to an int before returning it, with the test performing\n",
        "integer comparisons). - *Program Synthesis with Large Language Models*\n",
        "\n",
        "- The above script can be used to finetune T5 on any of the [CodeXGLUE](https://paperswithcode.com/dataset/codexglue) benchmark by microsoft. In CodeXGLUE we have language understanding tasks - clone defect detection & clone detection or generation tasks - code summarization, generation, translation, and refinement. To learn more check [here](https://microsoft.github.io/CodeXGLUE/) or [here](https://www.microsoft.com/en-us/research/blog/codexglue-a-benchmark-dataset-and-open-challenge-for-code-intelligence/)\n",
        "\n",
        "An AI powered coding assistant can be used with three code intelligence capabilities:\n",
        "\n",
        "1. *Text-to-code generation*: generate code based on the natural language description\n",
        "2. *Code autocompletion*: complete the whole function of code given the target function name\n",
        "3. *Code summarization*: generate the summary of a function in natural language description\n",
        "    \n",
        "For the first two functionalities, developers could simply type the natural language description or the function signature to specify their intents, and our AI coding assistant can generate or complete the target function for them. This helps to accelerate their implementation and also reduce their reliance on external resources. For code summarization, it can automatically summarize a function into code comments, which enables faster documentation and easier software maintenance.\n",
        "\n",
        "\n",
        "<a id=\"section14\"><font color='#FF6F00'><h2>References</h2></font></a>\n",
        "An acknowledgement to sources who made this notebook possible,\n",
        "\n",
        "- [CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation.](https://arxiv.org/pdf/2109.00859.pdf)\n",
        "- [CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation Blog.](https://blog.salesforceairesearch.com/codet5/)\n",
        "- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683)\n",
        "- [Codebert: A pre-trained model for programming and natural languages.](https://arxiv.org/pdf/2002.08155.pdf)\n",
        "- [CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation.](https://arxiv.org/pdf/2102.04664.pdf)\n",
        "- [Unified pre-training for program understanding and generation.](https://arxiv.org/pdf/2107.03374.pdf)\n",
        "- [Evaluating large language models trained on code.](https://arxiv.org/pdf/2107.03374.pdf)\n",
        "- [text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/tree/main/t5)\n",
        "- [TensorFlow Tutorials](https://www.tensorflow.org/tutorials)\n",
        "- [TensorFlow Guide](https://www.tensorflow.org/tutorials)\n",
        "- [HuggingFace Transformers Docs](https://huggingface.co/docs/transformers/index)\n",
        "- [HuggingFace Transformers](https://github.com/huggingface/transformers)\n",
        "- [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)\n",
        "- [Guiding Text Generation with Constrained Beam Search in ü§ó Transformers](https://huggingface.co/blog/constrained-beam-search)\n",
        "- [Fine_tune_CodeT5_for_generating_docstrings_from_Ruby_code](https://github.com/huggingface/transformers)\n",
        "- [Speeding up Transformer w/ Optimization Strategies](https://www.kaggle.com/code/rhtsingh/speeding-up-transformer-w-optimization-strategies)\n",
        "- [All The Ways You Can Compress Transformers](https://www.kaggle.com/code/rhtsingh/all-the-ways-you-can-compress-transformers)\n",
        "\n",
        "<font color='#FF6F00'><h2>Thank You!</h2></font>"
      ],
      "metadata": {
        "id": "KKMY_NtE8dYX"
      }
    }
  ]
}